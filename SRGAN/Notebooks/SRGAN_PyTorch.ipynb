{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SagarJiyani3010/GAN-Tutorials/blob/SRGAN/SRGAN/Notebooks/SRGAN_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEg5pW-QXHXx"
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEBrfGBMB1R0"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4C3QkZ6CGPP"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeIaZlz6CLJB"
      },
      "source": [
        "! kaggle datasets download -d akhileshdkapse/super-image-resolution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjcJ70FgCXTK"
      },
      "source": [
        "! unzip super-image-resolution.zip -d /content/drive/MyDrive/ColabNotebooks/GANs/SRGAN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwTKlJFGtQM4"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFQCUT6KtPMo",
        "outputId": "a9afa75b-3be6-4fc4-bfa5-5dd3f0395021"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import math\n",
        "from torchvision.models.vgg import vgg16\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9a68e5bc10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up03xkzKtvl_"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp7DPA8lCsWg"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, up_scale):\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels*up_scale**2, kernel_size=3, padding=1),\n",
        "            nn.PixelShuffle(up_scale),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, scale_factor):\n",
        "        upsample_block_num = int(math.log(scale_factor, 2))\n",
        "        super(Generator, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.block2 = ResidualBlock(64)\n",
        "        self.block3 = ResidualBlock(64)\n",
        "        self.block4 = ResidualBlock(64)\n",
        "        self.block5 = ResidualBlock(64)\n",
        "        self.block6 = ResidualBlock(64)\n",
        "        self.block7 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        block8 = [UpsampleBlock(64, 2) for _ in range(upsample_block_num)]\n",
        "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
        "        self.block8 = nn.Sequential(*block8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(block1)\n",
        "        block3 = self.block3(block2)\n",
        "        block4 = self.block4(block3)\n",
        "        block5 = self.block5(block4)\n",
        "        block6 = self.block6(block5)\n",
        "        block7 = self.block7(block6)\n",
        "        block8 = self.block8(block1 + block7)\n",
        "\n",
        "        return (torch.tanh(block8) + 1) / 2   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOGxWG9jt5rd"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7weTcott1gA"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(512, 1024, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(1024, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        return torch.sigmoid(self.net(x).view(batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJbjfJnRuCHl"
      },
      "source": [
        "# Loss FUnctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-wwMUKCt-Ug"
      },
      "source": [
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        vgg = vgg16(pretrained=True)\n",
        "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
        "        for param in loss_network.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.loss_network = loss_network\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        # Adversarial Loss\n",
        "        adversarial_loss = torch.mean(1 - out_labels)\n",
        "        # Perception Loss\n",
        "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
        "        # Image Loss\n",
        "        image_loss = self.mse_loss(out_images, target_images)\n",
        "        # TV Loss\n",
        "        tv_loss = self.tv_loss(out_images)\n",
        "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_loss_weight=1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.tv_loss_weight = tv_loss_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_size(t):\n",
        "        return t.size()[1] * t.size()[2] * t.size()[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll838aUCuHku"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GZiHXvuECw"
      },
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "INPUT_DIR = '/content/drive/MyDrive/ColabNotebooks/GANs/SRGAN/Data/LR'\n",
        "TARGET_DIR = '/content/drive/MyDrive/ColabNotebooks/GANs/SRGAN/Data/HR'\n",
        "# INPUT_DIR_TEST = '../data/LR_test/'\n",
        "# TARGET_DIR_TEST = '../data/HR_test/'\n",
        "UPSCALE_FACTOR = 4\n",
        "CROP_SIZE = 88\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "LEARNING_RATE = 0.0002\n",
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 2\n",
        "NUM_EPOCHS = 150\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = False\n",
        "CHECKPOINT_DISC = \"disc.pth.tar\"\n",
        "CHECKPOINT_GEN = \"gen.pth.tar\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X71ZNozuW4V"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxmKHjNkuJWr"
      },
      "source": [
        "class MapDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.input_files = os.listdir(self.input_dir)\n",
        "        self.target_files = os.listdir(self.target_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_files)\n",
        "\n",
        "    def __getitem__(self, index=0):\n",
        "        input_img_file = self.input_files[index]\n",
        "        input_img_path = os.path.join(self.input_dir, input_img_file)\n",
        "        input_image = Image.open(input_img_path)\n",
        "        input_image = input_image.resize((64 , 64))\n",
        "        input_image = transform(input_image)\n",
        "\n",
        "        target_img_file = self.target_files[index]\n",
        "        target_img_path = os.path.join(self.target_dir, target_img_file)\n",
        "        target_image = Image.open(target_img_path)\n",
        "        target_image = target_image.resize((256 , 256))\n",
        "        target_image = transform(target_image)\n",
        "\n",
        "        return input_image, target_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBGOm2dQuw3I"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqMyWfI9ubIa",
        "outputId": "dcf407cc-acab-4394-8be1-c9ba79f5f4a1"
      },
      "source": [
        "def train_fn(disc, gen, loader, generator_criterion, opt_disc, opt_gen, epoch):\n",
        "    train_bar = tqdm(loader)\n",
        "    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}        \n",
        "    \n",
        "    gen.train()\n",
        "    disc.train()\n",
        "\n",
        "    for data, target in train_bar:\n",
        "        g_update_first = True\n",
        "        batch_size = data.size(0)\n",
        "        running_results['batch_sizes'] += batch_size\n",
        "\n",
        "        real_img = Variable(target)\n",
        "        if torch.cuda.is_available():\n",
        "            real_img = real_img.cuda()\n",
        "        z = Variable(data)\n",
        "        if torch.cuda.is_available():\n",
        "            z = z.cuda()\n",
        "\n",
        "        # Update Discriminator Network\n",
        "        fake_img = gen(z)\n",
        "\n",
        "        disc.zero_grad()\n",
        "        real_out = disc(real_img).mean()\n",
        "        fake_out = disc(fake_img).mean()\n",
        "        d_loss = 1 - real_out + fake_out\n",
        "        d_loss.backward(retain_graph=True)\n",
        "        opt_disc.step()\n",
        "\n",
        "         # Update Generator network\n",
        "        fake_img = gen(z)\n",
        "        fake_out = disc(fake_img).mean()\n",
        "\n",
        "        gen.zero_grad()\n",
        "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
        "        g_loss.backward()\n",
        "\n",
        "        fake_img = gen(z)\n",
        "        fake_out = disc(fake_img).mean()\n",
        "\n",
        "        opt_gen.step()\n",
        "\n",
        "        running_results['g_loss'] += g_loss.item() * batch_size\n",
        "        running_results['d_loss'] += d_loss.item() * batch_size\n",
        "        running_results['d_score'] += real_out.item() * batch_size\n",
        "        running_results['g_score'] += fake_out.item() * batch_size\n",
        "\n",
        "        train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
        "            epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
        "            running_results['g_loss'] / running_results['batch_sizes'],\n",
        "            running_results['d_score'] / running_results['batch_sizes'],\n",
        "            running_results['g_score'] / running_results['batch_sizes']))\n",
        "\n",
        "    gen.eval()\n",
        "    out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
        "    if not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)\n",
        "\n",
        "\n",
        "def main():\n",
        "    disc = Discriminator().to(DEVICE)\n",
        "    gen = Generator(UPSCALE_FACTOR).to(DEVICE)\n",
        "    generator_criterion = GeneratorLoss().to(DEVICE)\n",
        "\n",
        "    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE)\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
        "\n",
        "    train_dataset = MapDataset(INPUT_DIR, TARGET_DIR)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "    )\n",
        "    \n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(disc, gen, train_loader, generator_criterion, opt_disc, opt_gen, epoch)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/150] Loss_D: 0.9104 Loss_G: 0.0810 D(x): 0.4783 D(G(z)): 0.3497: 100%|██████████| 7/7 [00:17<00:00,  2.46s/it]\n",
            "[1/150] Loss_D: 0.6344 Loss_G: 0.0816 D(x): 0.5888 D(G(z)): 0.2043: 100%|██████████| 7/7 [00:17<00:00,  2.48s/it]\n",
            "[2/150] Loss_D: 0.3961 Loss_G: 0.0809 D(x): 0.7440 D(G(z)): 0.1300: 100%|██████████| 7/7 [00:17<00:00,  2.51s/it]\n",
            "[3/150] Loss_D: 0.2238 Loss_G: 0.0791 D(x): 0.8825 D(G(z)): 0.0982: 100%|██████████| 7/7 [00:17<00:00,  2.55s/it]\n",
            "[4/150] Loss_D: 0.1091 Loss_G: 0.0788 D(x): 0.9418 D(G(z)): 0.0449: 100%|██████████| 7/7 [00:17<00:00,  2.56s/it]\n",
            "[5/150] Loss_D: 0.0597 Loss_G: 0.0790 D(x): 0.9652 D(G(z)): 0.0232: 100%|██████████| 7/7 [00:18<00:00,  2.58s/it]\n",
            "[6/150] Loss_D: 0.0436 Loss_G: 0.0785 D(x): 0.9785 D(G(z)): 0.0198: 100%|██████████| 7/7 [00:18<00:00,  2.60s/it]\n",
            "[7/150] Loss_D: 0.0285 Loss_G: 0.0777 D(x): 0.9855 D(G(z)): 0.0124: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[8/150] Loss_D: 0.0152 Loss_G: 0.0783 D(x): 0.9935 D(G(z)): 0.0084: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[9/150] Loss_D: 0.0108 Loss_G: 0.0778 D(x): 0.9957 D(G(z)): 0.0060: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[10/150] Loss_D: 0.0070 Loss_G: 0.0773 D(x): 0.9969 D(G(z)): 0.0037: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[11/150] Loss_D: 0.0052 Loss_G: 0.0776 D(x): 0.9977 D(G(z)): 0.0028: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[12/150] Loss_D: 0.0041 Loss_G: 0.0768 D(x): 0.9981 D(G(z)): 0.0022: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[13/150] Loss_D: 0.0035 Loss_G: 0.0768 D(x): 0.9985 D(G(z)): 0.0019: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[14/150] Loss_D: 0.0030 Loss_G: 0.0772 D(x): 0.9987 D(G(z)): 0.0017: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[15/150] Loss_D: 0.0026 Loss_G: 0.0765 D(x): 0.9989 D(G(z)): 0.0014: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[16/150] Loss_D: 0.0024 Loss_G: 0.0760 D(x): 0.9990 D(G(z)): 0.0014: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[17/150] Loss_D: 0.0021 Loss_G: 0.0774 D(x): 0.9991 D(G(z)): 0.0012: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[18/150] Loss_D: 0.0019 Loss_G: 0.0763 D(x): 0.9992 D(G(z)): 0.0010: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[19/150] Loss_D: 0.0017 Loss_G: 0.0761 D(x): 0.9992 D(G(z)): 0.0009: 100%|██████████| 7/7 [00:18<00:00,  2.66s/it]\n",
            "[20/150] Loss_D: 0.0017 Loss_G: 0.0766 D(x): 0.9993 D(G(z)): 0.0010: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[21/150] Loss_D: 0.0014 Loss_G: 0.0752 D(x): 0.9994 D(G(z)): 0.0008: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[22/150] Loss_D: 0.0014 Loss_G: 0.0769 D(x): 0.9994 D(G(z)): 0.0008: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[23/150] Loss_D: 0.0012 Loss_G: 0.0750 D(x): 0.9994 D(G(z)): 0.0007: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[24/150] Loss_D: 0.0012 Loss_G: 0.0760 D(x): 0.9995 D(G(z)): 0.0007: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[25/150] Loss_D: 0.0011 Loss_G: 0.0749 D(x): 0.9995 D(G(z)): 0.0006: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[26/150] Loss_D: 0.0011 Loss_G: 0.0753 D(x): 0.9996 D(G(z)): 0.0006: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[27/150] Loss_D: 0.0009 Loss_G: 0.0768 D(x): 0.9996 D(G(z)): 0.0005: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[28/150] Loss_D: 0.0009 Loss_G: 0.0764 D(x): 0.9996 D(G(z)): 0.0005: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[29/150] Loss_D: 0.0008 Loss_G: 0.0766 D(x): 0.9996 D(G(z)): 0.0004: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[30/150] Loss_D: 0.0008 Loss_G: 0.0738 D(x): 0.9996 D(G(z)): 0.0004: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[31/150] Loss_D: 0.0007 Loss_G: 0.0742 D(x): 0.9997 D(G(z)): 0.0004: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[32/150] Loss_D: 0.0007 Loss_G: 0.0736 D(x): 0.9997 D(G(z)): 0.0004: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[33/150] Loss_D: 0.0006 Loss_G: 0.0737 D(x): 0.9997 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[34/150] Loss_D: 0.0006 Loss_G: 0.0739 D(x): 0.9997 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[35/150] Loss_D: 0.0006 Loss_G: 0.0735 D(x): 0.9997 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[36/150] Loss_D: 0.0005 Loss_G: 0.0731 D(x): 0.9997 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[37/150] Loss_D: 0.0005 Loss_G: 0.0724 D(x): 0.9997 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[38/150] Loss_D: 0.0005 Loss_G: 0.0748 D(x): 0.9998 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[39/150] Loss_D: 0.0005 Loss_G: 0.0729 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[40/150] Loss_D: 0.0004 Loss_G: 0.0733 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[41/150] Loss_D: 0.0004 Loss_G: 0.0722 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[42/150] Loss_D: 0.0004 Loss_G: 0.0722 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[43/150] Loss_D: 0.0004 Loss_G: 0.0726 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[44/150] Loss_D: 0.0004 Loss_G: 0.0724 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[45/150] Loss_D: 0.0004 Loss_G: 0.0725 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[46/150] Loss_D: 0.0004 Loss_G: 0.0718 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[47/150] Loss_D: 0.0004 Loss_G: 0.0714 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[48/150] Loss_D: 0.0003 Loss_G: 0.0716 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[49/150] Loss_D: 0.0003 Loss_G: 0.0706 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[50/150] Loss_D: 0.0003 Loss_G: 0.0704 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[51/150] Loss_D: 0.0003 Loss_G: 0.0699 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[52/150] Loss_D: 0.0003 Loss_G: 0.0701 D(x): 0.9998 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[53/150] Loss_D: 0.0003 Loss_G: 0.0695 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[54/150] Loss_D: 0.0003 Loss_G: 0.0701 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[55/150] Loss_D: 0.0003 Loss_G: 0.0699 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[56/150] Loss_D: 0.0002 Loss_G: 0.0706 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[57/150] Loss_D: 0.0003 Loss_G: 0.0700 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[58/150] Loss_D: 0.0002 Loss_G: 0.0688 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[59/150] Loss_D: 0.0002 Loss_G: 0.0694 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[60/150] Loss_D: 0.0002 Loss_G: 0.0703 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[61/150] Loss_D: 0.0002 Loss_G: 0.0687 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[62/150] Loss_D: 0.0002 Loss_G: 0.0702 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[63/150] Loss_D: 0.0002 Loss_G: 0.0695 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[64/150] Loss_D: 0.0002 Loss_G: 0.0683 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[65/150] Loss_D: 0.0002 Loss_G: 0.0690 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[66/150] Loss_D: 0.0002 Loss_G: 0.0682 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[67/150] Loss_D: 0.0002 Loss_G: 0.0696 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[68/150] Loss_D: 0.0002 Loss_G: 0.0683 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[69/150] Loss_D: 0.0002 Loss_G: 0.0689 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[70/150] Loss_D: 0.0002 Loss_G: 0.0668 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[71/150] Loss_D: 0.0002 Loss_G: 0.0684 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[72/150] Loss_D: 0.0002 Loss_G: 0.0681 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[73/150] Loss_D: 0.0002 Loss_G: 0.0691 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.65s/it]\n",
            "[74/150] Loss_D: 0.0001 Loss_G: 0.0680 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[75/150] Loss_D: 0.0001 Loss_G: 0.0670 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[76/150] Loss_D: 0.0002 Loss_G: 0.0682 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[77/150] Loss_D: 0.0001 Loss_G: 0.0695 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[78/150] Loss_D: 0.0001 Loss_G: 0.0682 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[79/150] Loss_D: 0.0001 Loss_G: 0.0679 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[80/150] Loss_D: 0.0001 Loss_G: 0.0689 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[81/150] Loss_D: 0.0001 Loss_G: 0.0671 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[82/150] Loss_D: 0.0001 Loss_G: 0.0677 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[83/150] Loss_D: 0.0001 Loss_G: 0.0669 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[84/150] Loss_D: 0.0001 Loss_G: 0.0661 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[85/150] Loss_D: 0.0003 Loss_G: 0.0660 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[86/150] Loss_D: 0.0726 Loss_G: 0.0657 D(x): 0.9635 D(G(z)): 0.0526: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[87/150] Loss_D: 0.9665 Loss_G: 0.0655 D(x): 0.8438 D(G(z)): 0.7990: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[88/150] Loss_D: 1.0619 Loss_G: 0.0654 D(x): 0.6592 D(G(z)): 0.6978: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[89/150] Loss_D: 1.0531 Loss_G: 0.0671 D(x): 0.7409 D(G(z)): 0.7690: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[90/150] Loss_D: 0.8763 Loss_G: 0.0652 D(x): 0.7168 D(G(z)): 0.5816: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[91/150] Loss_D: 0.8319 Loss_G: 0.0657 D(x): 0.7540 D(G(z)): 0.5704: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[92/150] Loss_D: 0.8549 Loss_G: 0.0649 D(x): 0.7046 D(G(z)): 0.5496: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[93/150] Loss_D: 0.8759 Loss_G: 0.0652 D(x): 0.6887 D(G(z)): 0.5615: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[94/150] Loss_D: 0.8154 Loss_G: 0.0651 D(x): 0.7266 D(G(z)): 0.5536: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[95/150] Loss_D: 0.8522 Loss_G: 0.0666 D(x): 0.7229 D(G(z)): 0.5434: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[96/150] Loss_D: 0.7728 Loss_G: 0.0661 D(x): 0.6589 D(G(z)): 0.4148: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[97/150] Loss_D: 0.7888 Loss_G: 0.0670 D(x): 0.6307 D(G(z)): 0.4223: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[98/150] Loss_D: 0.7645 Loss_G: 0.0673 D(x): 0.7087 D(G(z)): 0.4657: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[99/150] Loss_D: 0.7653 Loss_G: 0.0653 D(x): 0.7371 D(G(z)): 0.4927: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[100/150] Loss_D: 0.7588 Loss_G: 0.0662 D(x): 0.6666 D(G(z)): 0.3866: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[101/150] Loss_D: 0.8495 Loss_G: 0.0671 D(x): 0.6457 D(G(z)): 0.4552: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[102/150] Loss_D: 0.7687 Loss_G: 0.0648 D(x): 0.6445 D(G(z)): 0.3927: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[103/150] Loss_D: 0.8308 Loss_G: 0.0647 D(x): 0.5536 D(G(z)): 0.3601: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[104/150] Loss_D: 0.7859 Loss_G: 0.0652 D(x): 0.5398 D(G(z)): 0.3223: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[105/150] Loss_D: 0.7335 Loss_G: 0.0651 D(x): 0.5996 D(G(z)): 0.3196: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[106/150] Loss_D: 0.7749 Loss_G: 0.0662 D(x): 0.6268 D(G(z)): 0.4001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[107/150] Loss_D: 0.6641 Loss_G: 0.0643 D(x): 0.6623 D(G(z)): 0.3186: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[108/150] Loss_D: 0.6493 Loss_G: 0.0639 D(x): 0.7317 D(G(z)): 0.3763: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[109/150] Loss_D: 0.6028 Loss_G: 0.0634 D(x): 0.7835 D(G(z)): 0.3470: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[110/150] Loss_D: 0.6253 Loss_G: 0.0649 D(x): 0.7345 D(G(z)): 0.3601: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[111/150] Loss_D: 0.5263 Loss_G: 0.0635 D(x): 0.7850 D(G(z)): 0.2754: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[112/150] Loss_D: 0.4527 Loss_G: 0.0640 D(x): 0.8219 D(G(z)): 0.2679: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[113/150] Loss_D: 0.5708 Loss_G: 0.0652 D(x): 0.7783 D(G(z)): 0.2927: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[114/150] Loss_D: 0.7367 Loss_G: 0.0652 D(x): 0.7806 D(G(z)): 0.5177: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[115/150] Loss_D: 0.7457 Loss_G: 0.0661 D(x): 0.8455 D(G(z)): 0.5328: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[116/150] Loss_D: 0.7659 Loss_G: 0.0660 D(x): 0.6551 D(G(z)): 0.3549: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[117/150] Loss_D: 0.6557 Loss_G: 0.0634 D(x): 0.6785 D(G(z)): 0.3346: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[118/150] Loss_D: 0.4740 Loss_G: 0.0630 D(x): 0.9432 D(G(z)): 0.3744: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[119/150] Loss_D: 0.4887 Loss_G: 0.0626 D(x): 0.9341 D(G(z)): 0.3672: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[120/150] Loss_D: 0.4367 Loss_G: 0.0639 D(x): 0.9275 D(G(z)): 0.3130: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[121/150] Loss_D: 0.3072 Loss_G: 0.0642 D(x): 0.9776 D(G(z)): 0.2657: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[122/150] Loss_D: 0.3261 Loss_G: 0.0644 D(x): 0.9935 D(G(z)): 0.2898: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[123/150] Loss_D: 0.3479 Loss_G: 0.0636 D(x): 0.8995 D(G(z)): 0.2224: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[124/150] Loss_D: 0.2706 Loss_G: 0.0636 D(x): 0.9989 D(G(z)): 0.2399: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[125/150] Loss_D: 0.4908 Loss_G: 0.0643 D(x): 0.6889 D(G(z)): 0.2069: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[126/150] Loss_D: 0.5546 Loss_G: 0.0649 D(x): 0.8188 D(G(z)): 0.3126: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[127/150] Loss_D: 0.3547 Loss_G: 0.0659 D(x): 0.9125 D(G(z)): 0.2622: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[128/150] Loss_D: 0.4122 Loss_G: 0.0635 D(x): 0.9253 D(G(z)): 0.2927: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[129/150] Loss_D: 0.4736 Loss_G: 0.0634 D(x): 0.8946 D(G(z)): 0.3571: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[130/150] Loss_D: 0.3099 Loss_G: 0.0633 D(x): 0.9479 D(G(z)): 0.2463: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[131/150] Loss_D: 0.2775 Loss_G: 0.0651 D(x): 0.9865 D(G(z)): 0.2281: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[132/150] Loss_D: 0.1935 Loss_G: 0.0659 D(x): 0.9798 D(G(z)): 0.1714: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[133/150] Loss_D: 0.2208 Loss_G: 0.0634 D(x): 0.9843 D(G(z)): 0.2053: 100%|██████████| 7/7 [00:18<00:00,  2.66s/it]\n",
            "[134/150] Loss_D: 0.1775 Loss_G: 0.0628 D(x): 0.9942 D(G(z)): 0.1611: 100%|██████████| 7/7 [00:18<00:00,  2.66s/it]\n",
            "[135/150] Loss_D: 0.2247 Loss_G: 0.0643 D(x): 0.9595 D(G(z)): 0.1663: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[136/150] Loss_D: 0.1263 Loss_G: 0.0637 D(x): 0.9984 D(G(z)): 0.1220: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[137/150] Loss_D: 0.1579 Loss_G: 0.0637 D(x): 0.9985 D(G(z)): 0.1447: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[138/150] Loss_D: 0.2695 Loss_G: 0.0626 D(x): 0.8660 D(G(z)): 0.1311: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[139/150] Loss_D: 0.1828 Loss_G: 0.0627 D(x): 0.9848 D(G(z)): 0.1679: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[140/150] Loss_D: 0.2482 Loss_G: 0.0630 D(x): 0.9652 D(G(z)): 0.1632: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[141/150] Loss_D: 0.2199 Loss_G: 0.0630 D(x): 0.9232 D(G(z)): 0.1426: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[142/150] Loss_D: 0.1379 Loss_G: 0.0643 D(x): 0.9935 D(G(z)): 0.1336: 100%|██████████| 7/7 [00:18<00:00,  2.65s/it]\n",
            "[143/150] Loss_D: 0.1856 Loss_G: 0.0628 D(x): 0.9966 D(G(z)): 0.1215: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[144/150] Loss_D: 0.1229 Loss_G: 0.0640 D(x): 0.9146 D(G(z)): 0.0330: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[145/150] Loss_D: 0.0202 Loss_G: 0.0632 D(x): 0.9896 D(G(z)): 0.0021: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[146/150] Loss_D: 0.0146 Loss_G: 0.0619 D(x): 0.9974 D(G(z)): 0.0111: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[147/150] Loss_D: 0.0042 Loss_G: 0.0618 D(x): 0.9991 D(G(z)): 0.0021: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[148/150] Loss_D: 0.0029 Loss_G: 0.0634 D(x): 0.9985 D(G(z)): 0.0007: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[149/150] Loss_D: 0.0026 Loss_G: 0.0621 D(x): 0.9986 D(G(z)): 0.0007: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XDAoP1juvos"
      },
      "source": [
        "torch.save(gen.state_dict(), \"super_res_gen.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGJyVq9Qwzr1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}